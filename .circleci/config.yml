version: 2.1

workflows:
  version: 2
  run-checks:
    jobs:
      - linter
      - analytics-tests
      - recommendation-db-install
      - recommendation-db-tests:
          requires:
            - recommendation-db-install
      - recommendation-api-build
      - recommendation-api-tests:
          requires:
            - recommendation-api-build
      - orchestration-install
      - orchestration-tests:
          requires:
            - orchestration-install
      - ai-platform-deploy:
          filters:
            branches:
              only:
                - master
      - composer-deploy:
          requires:
            - orchestration-tests
            - ai-platform-deploy
          filters:
            branches:
              only:
                - master

defaults: &defaults
  docker:
    - image: circleci/python:3.7
      auth:
        username: $DOCKERHUB_USER
        password: $DOCKERHUB_PASSWORD

jobs:
  analytics-tests:
    <<: *defaults
    working_directory: ~/data-gcp/analytics
    steps:
      - checkout:
          path: ~/data-gcp
      - restore_cache:
          key: deps-v2-{{ checksum "analytics-requirements.txt" }}
      - run:
          name: Install python dependencies
          command: |
            python3 -m venv venv-analytics
            . venv-analytics/bin/activate
            pip install -r analytics-requirements.txt
      - save_cache:
          key: deps-v2-{{ checksum "analytics-requirements.txt" }}
          paths:
            - "venv-analytics"
      - run:
          name: GCP service account authentication
          command: |
            echo $GCLOUD_SERVICE_KEY >> ~/data-gcp/analytics/gcloud_credentials.json
            echo GOOGLE_APPLICATION_CREDENTIALS=~/data-gcp/analytics/gcloud_credentials.json >> ~/data-gcp/.env
      - run:
          name: Run tests
          command: |
            . venv-analytics/bin/activate
            export PYTHONPATH=$PYTHONPATH:~/data-gcp:~/data-gcp/orchestration/dags
            pytest tests

  recommendation-api-build:
    <<: *defaults
    working_directory: ~/data-gcp/recommendation/api
    steps:
      - checkout:
          path: ~/data-gcp
      - restore_cache:
          key: deps-{{ checksum "api-dev-requirements.txt" }}
      - run:
          name: Install Python deps in a venv
          command: |
            python3 -m venv venv-api
            . venv-api/bin/activate
            pip install -r api-dev-requirements.txt
      - save_cache:
          key: deps-{{ checksum "api-dev-requirements.txt" }}
          paths:
            - "venv-api"
      - persist_to_workspace:
          root: .
          paths:
            - .

  recommendation-api-tests:
    docker:
      - image: circleci/python:3.7
        auth:
          username: $DOCKERHUB_USER
          password: $DOCKERHUB_PASSWORD
      - image: kartoza/postgis:12.4
        environment:
          POSTGRES_PASS: postgres
          POSTGRES_USER: postgres
          POSTGRES_DBNAME: db
    working_directory: ~/data-gcp/recommendation/api
    steps:
      - attach_workspace:
          at: .
      - run:
          name: Install psql
          command: sudo apt install postgresql-client
      - run:
          name: Run tests
          command: |
            export PYTHONPATH=$PYTHONPATH:~/data-gcp/recommendation/api
            . venv-api/bin/activate
            ./run_tests.sh

  linter:
    <<: *defaults
    working_directory: ~/data-gcp
    steps:
      - checkout
      - restore_cache:
          key: deps-{{ checksum "linter-requirements.txt" }}
      - run:
          name: Install black
          command: |
            python3 -m venv venv-lint
            . venv-lint/bin/activate
            pip install -r linter-requirements.txt
      - save_cache:
          key: deps-{{ checksum "linter-requirements.txt" }}
          paths:
            - "venv-lint"
      - run:
          name: Run linter
          command: |
            . venv-lint/bin/activate
            black --exclude="venv-lint" --check .

  orchestration-install:
    <<: *defaults
    working_directory: ~/data-gcp/orchestration
    steps:
      - checkout:
          path: ~/data-gcp
      - restore_cache:
          key: deps-{{ checksum "orchestration-requirements.txt" }}
      - run:
          name: Install Python requirements
          command: |
            python3 -m venv venv-orchestration
            . venv-orchestration/bin/activate
            pip install -r orchestration-requirements.txt
      - save_cache:
          key: deps-{{ checksum "orchestration-requirements.txt" }}
          paths:
            - "venv-orchestration"
      - persist_to_workspace:
          root: .
          paths:
            - .

  orchestration-tests:
    <<: *defaults
    working_directory: ~/data-gcp/orchestration
    environment:
      - AIRFLOW_HOME=~/data-gcp/orchestration
      - DAG_FOLDER=~/data-gcp/orchestration/dags
      - RECOMMENDATION_SQL_USER=test_user
      - RECOMMENDATION_SQL_PASSWORD=test_password
      - RECOMMENDATION_SQL_PUBLIC_PORT=1234
      - RECOMMENDATION_SQL_PUBLIC_IP=11.111.111.111
    steps:
      - attach_workspace:
          at: .
      - run:
          name: Run tests
          command: |
            . venv-orchestration/bin/activate
            airflow initdb
            pytest tests

  recommendation-db-install:
    <<: *defaults
    working_directory: ~/data-gcp/recommendation/db
    steps:
      - checkout:
          path: ~/data-gcp
      - restore_cache:
          key: dependencies-{{ checksum "db-requirements.txt" }}
      - run:
          name: Install Python deps in a venv
          command: |
            python3 -m venv venv-db
            . venv-db/bin/activate
            pip install -r db-requirements.txt
      - save_cache:
          key: dependencies-{{ checksum "db-requirements.txt" }}
          paths:
            - "venv-db"
      - persist_to_workspace:
          root: .
          paths:
            - .

  recommendation-db-tests:
    docker:
      - image: circleci/python:3.7
        auth:
          username: $DOCKERHUB_USER
          password: $DOCKERHUB_PASSWORD
      - image: circleci/postgres:12
        environment:
          POSTGRES_PASSWORD: postgres
    working_directory: ~/data-gcp/recommendation/db
    steps:
      - attach_workspace:
          at: .
      - run:
          name: Install psql
          command: sudo apt install postgresql-client
      - run:
          name: Run tests
          command: |
            export PYTHONPATH=$PYTHONPATH:~/data-gcp/recommendation/db
            . venv-db/bin/activate
            ./run_tests.sh

  composer-deploy:
    docker:
      - image: google/cloud-sdk
        auth:
          username: $DOCKERHUB_USER
          password: $DOCKERHUB_PASSWORD
    environment:
      COMPOSER_BUCKET: gs://europe-west1-data-composer-0f2400f5-bucket/dags
      COMPOSER_ENV_NAME: data-composer
      COMPOSER_REGION: europe-west1
      DAG_RESTORE_DATA_ANALYTICS: restore_data_analytics_v1
      DAG_RECOMMMENDATION_CLOUDSQL: recommendation_cloud_sql_v42
      DAG_DUMP_SCALINGO: dump_scalingo_v1
      DAG_EXPORT_CLOUDSQL_BIGQUERY: export_cloudsql_tables_to_bigquery_v3
      DAG_IMPORT_APPLICATIVE_DATABASE: import_applicative_database_v1
      DAG_ARCHIVE_DATABASE: archive_database_v1
    working_directory: ~/data-gcp/orchestration
    steps:
      - checkout:
          path: ~/data-gcp
      - run:
          name: Connect to GCP
          command: |
            echo $GCLOUD_SERVICE_KEY | gcloud auth activate-service-account --key-file=-
            gcloud --quiet config set project ${GOOGLE_PROJECT_ID}
            gcloud --quiet config set compute/zone ${GOOGLE_COMPUTE_ZONE}
      - run:
          name: Deploy to composer
          command: |
            gsutil -m rsync -c -r dags ${COMPOSER_BUCKET}
      - run:
          name: Wait for dags to be deployed
          command: |
            ./wait_for_dag_deployed.sh ${COMPOSER_ENV_NAME} ${COMPOSER_REGION} ${DAG_RECOMMMENDATION_CLOUDSQL} 6 20
            ./wait_for_dag_deployed.sh ${COMPOSER_ENV_NAME} ${COMPOSER_REGION} ${DAG_RESTORE_DATA_ANALYTICS} 6 20
            ./wait_for_dag_deployed.sh ${COMPOSER_ENV_NAME} ${COMPOSER_REGION} ${DAG_DUMP_SCALINGO} 6 20
            ./wait_for_dag_deployed.sh ${COMPOSER_ENV_NAME} ${COMPOSER_REGION} ${DAG_EXPORT_CLOUDSQL_BIGQUERY} 6 20
            ./wait_for_dag_deployed.sh ${COMPOSER_ENV_NAME} ${COMPOSER_REGION} ${DAG_IMPORT_APPLICATIVE_DATABASE} 6 20
            ./wait_for_dag_deployed.sh ${COMPOSER_ENV_NAME} ${COMPOSER_REGION} ${DAG_ARCHIVE_DATABASE} 6 20

  ai-platform-deploy:
    docker:
      - image: google/cloud-sdk
        auth:
          username: $DOCKERHUB_USER
          password: $DOCKERHUB_PASSWORD
    environment:
      VERSION_NAME: latest
      MODEL_NAME: poc_model
      MODEL_DIR: gs://pass-culture-data/ai_platform
      FRAMEWORK: SCIKIT_LEARN
      REGION: europe-west1
      MACHINE_TYPE: n1-standard-2
    working_directory: ~/data-gcp
    steps:
      - checkout:
          path: ~/data-gcp
      - run:
          name: Connect to GCP
          command: |
            echo $GCLOUD_SERVICE_KEY | gcloud auth activate-service-account --key-file=-
            gcloud --quiet config set project ${GOOGLE_PROJECT_ID}
      - run:
          name: Deploy to Cloud Storage
          command: |
            gsutil cp recommendation/model/model.joblib $MODEL_DIR/model_test.joblib
      - run:
          name: Deploy to AI Platform
          command: |
            gcloud ai-platform versions delete $VERSION_NAME --model=$MODEL_NAME --region=$REGION
            gcloud ai-platform versions create $VERSION_NAME \
              --model=$MODEL_NAME \
              --origin=$MODEL_DIR \
              --runtime-version=2.2 \
              --framework=$FRAMEWORK \
              --python-version=3.7 \
              --region=$REGION \
              --machine-type=$MACHINE_TYPE
