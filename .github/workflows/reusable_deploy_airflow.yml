name: "Deploy Airflow"

run-name: "Deploy Airflow on ${{ github.event.inputs.GITHUB_ENV_NAME }}"

on:
  workflow_call:
    inputs:
      DATA_GCP_PROJECT:
        type: string
        required: true
      AIRFLOW_DAGS_BUCKET:
        type: string
        required: true
      AIRFLOW_NAME:
        type: string
        required: true
      GITHUB_ENV_NAME:
        type: string
        required: true
      ENV_SHORT_NAME:
        type: string
        required: true
      APPLICATIVE_EXTERNAL_CONNECTION_ID:
        type: string
        required: false
      IS_COMPOSER:
        type: boolean
        required: true
        description: "Temporary; If false, the workflow will not wait for the composer to be deployed as we are not deploying a composer but custom airflow"
    secrets:
      GCP_EHP_WORKLOAD_IDENTITY_PROVIDER:
        required: true
      GCP_EHP_SERVICE_ACCOUNT:
        required: true

env:
  GCP_REGION: "europe-west1"

jobs:
  airflow-deploy:
    permissions:
      id-token: write
      contents: write
    runs-on: ubuntu-latest
    environment: ${{ inputs.GITHUB_ENV_NAME }}
    defaults:
      run:
        working-directory: "./orchestration/"
    steps:
      - uses: actions/checkout@v4
      - name: "Connect to Secret Manager"
        uses: "google-github-actions/auth@v2"
        with:
          workload_identity_provider: ${{ secrets.GCP_EHP_WORKLOAD_IDENTITY_PROVIDER }}
          service_account: ${{ secrets.GCP_EHP_SERVICE_ACCOUNT }}
      - name: "Get secrets for Slack"
        id: "secrets"
        uses: "google-github-actions/get-secretmanager-secrets@v2"
        with:
          secrets: |-
            SLACK_BOT_TOKEN:passculture-metier-ehp/passculture-ci-slack-bot-token
            ARTIFACT_REGISTRY_SERVICE_ACCOUNT:passculture-metier-ehp/passculture-main-artifact-registry-service-account
            ARTIFACT_REGISTRY_WORKLOAD_IDENTITY_PROVIDER:passculture-metier-ehp/infra-prod-gcp-workload-identity-provider
      - uses: actions/setup-python@v4
        with:
          python-version: "3.10"
      - uses: yezz123/setup-uv@v4
      - name: "Install Python requirements"
        run: uv pip install --system -r airflow/orchestration-requirements.txt
      - id: auth
        name: "Authenticate with Google Cloud"
        uses: "google-github-actions/auth@v2"
        with:
          create_credentials_file: true
          project_id: ${{ inputs.DATA_GCP_PROJECT }}
          token_format: "access_token"
          workload_identity_provider: ${{ steps.secrets.outputs.ARTIFACT_REGISTRY_WORKLOAD_IDENTITY_PROVIDER }}
          service_account: ${{ steps.secrets.outputs.ARTIFACT_REGISTRY_SERVICE_ACCOUNT }}
      - name: "Set up Cloud SDK"
        uses: "google-github-actions/setup-gcloud@v2"

      - name: "Pull dbt exposures (if any) from GCS locally"
        working-directory: "./orchestration/dags/data_gcp_dbt/"
        env:
          AIRFLOW_DAGS_BUCKET: ${{ inputs.AIRFLOW_DAGS_BUCKET }}
        run: gsutil cp 'gs://${{ inputs.AIRFLOW_DAGS_BUCKET }}/data/exposures/*' ./models/

      - name: "Install dbt dependencies"
        working-directory: "./orchestration/dags/data_gcp_dbt/"
        env:
          DBT_TARGET_PATH: "target/"
          DBT_PROFILES_DIR: "."
        run: |
          export PYTHONPATH=$PYTHONPATH:./dags
          dbt deps

      - name: "compile dbt & generate dbt docs"
        working-directory: "./orchestration/dags/data_gcp_dbt/"
        env:
          DBT_TARGET_PATH: "target/"
          DBT_PROFILES_DIR: "."
          ENV_SHORT_NAME: ${{ inputs.ENV_SHORT_NAME }}
          APPLICATIVE_EXTERNAL_CONNECTION_ID: ${{ inputs.APPLICATIVE_EXTERNAL_CONNECTION_ID }}
        run: dbt docs generate --static --target $ENV_SHORT_NAME --vars "{'ENV_SHORT_NAME':'$ENV_SHORT_NAME'}"

      - name: "Generate dbt colibri documentation"
        working-directory: "./orchestration/dags/data_gcp_dbt/"
        env:
          DBT_TARGET_PATH: "target/"
          DBT_PROFILES_DIR: "."
          ENV_SHORT_NAME: ${{ inputs.ENV_SHORT_NAME }}
        run: uv run colibri generate

      - name: "Push dependencies (dbt) to GCS (data/target/dbt_packages)"
        working-directory: "./orchestration/dags/data_gcp_dbt/"
        env:
          AIRFLOW_DAGS_BUCKET: ${{ inputs.AIRFLOW_DAGS_BUCKET }}
        run: gsutil -m rsync -d -r target/dbt_packages gs://${{ inputs.AIRFLOW_DAGS_BUCKET }}/data/target/dbt_packages

      - name: "Push manifest (dbt) to GCS (data/target)"
        working-directory: "./orchestration/dags/data_gcp_dbt/"
        env:
          AIRFLOW_DAGS_BUCKET: ${{ inputs.AIRFLOW_DAGS_BUCKET }}
        run: gsutil cp target/manifest.json gs://$AIRFLOW_DAGS_BUCKET/data/target/manifest.json

      # Compute exclusion regex from .rsyncignore
      - name: "Compute EXCLUDE_REGEX from .rsyncignore file"
        id: compute_rsyncignore
        run: |
          readonly source_dir="dags"
          readonly ignore_file="${source_dir}/.rsyncignore"

          EXCLUDE_REGEX=""

          if [ -f "$ignore_file" ]; then
            echo "Found .rsyncignore file, building exclude regex..."

            # Read each non-empty, non-comment line and convert to regex
            while IFS= read -r pattern; do
              # Skip comments and empty lines
              [[ -z "$pattern" || "$pattern" =~ ^# ]] && continue

              # Escape regex special characters (except * which we'll handle below)
              safe_pattern=$(printf '%s' "$pattern" | sed 's/[.[\^$(){}+?|]/\\&/g')

              # Convert simple glob-style wildcards to regex
              safe_pattern=$(echo "$safe_pattern" | sed 's/\*/.*/g')

              # Ensure pattern is relative to source_dir
              EXCLUDE_REGEX="${EXCLUDE_REGEX}${source_dir}/${safe_pattern}|"
            done < "$ignore_file"

            # Remove trailing |
            EXCLUDE_REGEX="${EXCLUDE_REGEX%|}"
          fi

          # Export for next step
          echo "EXCLUDE_REGEX=$EXCLUDE_REGEX" >> $GITHUB_ENV
          echo "Final EXCLUDE_REGEX=$EXCLUDE_REGEX"
        shell: bash
      # Deploy DAGs to GCS, using computed EXCLUDE_REGEX
      - name: "Deploy Airflow (dags)"
        env:
          AIRFLOW_DAGS_BUCKET: ${{ inputs.AIRFLOW_DAGS_BUCKET }}
        run: |
          readonly bucket="$AIRFLOW_DAGS_BUCKET"
          readonly source_dir="dags"

          echo "Deploying DAGs..."
          if [ -z "$EXCLUDE_REGEX" ]; then
            gsutil -m rsync -d -r "$source_dir" "gs://${bucket}/dags"
          else
            echo "Excluding folders matching: $EXCLUDE_REGEX"
            gsutil -m rsync -d -r -x "$EXCLUDE_REGEX" "$source_dir" "gs://${bucket}/dags"
          fi
        shell: bash

      - name: "Compute dbt lineage layout"
        run: |
          cat <<EOF > compute_layout.py
          import sys
          import yaml
          import json
          import os

          # Add plugin dir to path so we can import layout_algorithm
          sys.path.append('plugins/static/dbt_lineage')
          try:
              from layout_algorithm import compute_layout_from_viz_data
          except ImportError as e:
              print(f"Error importing layout_algorithm: {e}")
              sys.exit(1)

          # Helper to load config manually since layout_algorithm.py's loader
          # depends on AIRFLOW_HOME which isn't set here
          def load_config():
              print("Loading config from plugins/static/dbt_lineage/config.yaml")
              with open('plugins/static/dbt_lineage/config.yaml') as f:
                  yaml_config = yaml.safe_load(f)

              # Extract dimensions
              dimensions = yaml_config.get("project", {}).get("dimensions", {})
              config = {
                  "width": dimensions.get("width", 1920),
                  "height": dimensions.get("height", 1080),
              }

              # Extract force parameters
              forces = yaml_config.get("forces", {})
              for key, params in forces.items():
                  if isinstance(params, dict) and "val" in params:
                      config[key] = params["val"]

              # Extract disabled forces
              config["disabled_forces"] = {
                  force: list(node_ids)
                  for force, node_ids in yaml_config.get("disabled_forces", {}).items()
              }
              return config

          try:
              config = load_config()

              input_path = 'plugins/static/dbt_lineage/viz_data.json'
              output_path = 'plugins/static/dbt_lineage/viz_data_with_layout.json'

              if not os.path.exists(input_path):
                  print(f"Warning: {input_path} not found. Skipping layout computation.")
                  sys.exit(0)

              print(f"Reading from {input_path}")
              with open(input_path, 'r') as f:
                  viz_data = json.load(f)

              print(f"Computing layout for {len(viz_data.get('nodes', []))} nodes...")
              viz_data = compute_layout_from_viz_data(viz_data, config)

              print(f"Writing to {output_path}")
              with open(output_path, 'w') as f:
                  json.dump(viz_data, f, indent=2)
              print("Layout computation complete.")

          except Exception as e:
              print(f"Error computing layout: {e}")
              sys.exit(1)
          EOF

          python compute_layout.py
          rm compute_layout.py

      - name: "Deploy Airflow (plugins)"
        run: gsutil -m rsync -d -r plugins gs://${{ inputs.AIRFLOW_DAGS_BUCKET }}/plugins

      - name: "Push documentation (dbt) to GCS (plugins/static/dbt_*)"
        working-directory: "./orchestration/dags/data_gcp_dbt/"
        env:
          AIRFLOW_DAGS_BUCKET: ${{ inputs.AIRFLOW_DAGS_BUCKET }}
        run: |
          gsutil cp target/static_index.html gs://$AIRFLOW_DAGS_BUCKET/plugins/static/dbt_docs/index.html
          gsutil cp dist/index.html gs://$AIRFLOW_DAGS_BUCKET/plugins/static/dbt_docs/colibri.html

      - name: "Wait for composer to be deployed (10s x 6 times)"
        if: ${{ inputs.IS_COMPOSER }}
        run: |
          ./wait_for_dag_deployed.sh ${{ inputs.AIRFLOW_NAME }} ${{ env.GCP_REGION }} airflow_monitoring 6 10 ${{ inputs.DATA_GCP_PROJECT }}
      - name: "Post to a Slack channel"
        if: always()
        uses: slackapi/slack-github-action@v1.23.0
        env:
          SLACK_BOT_TOKEN: ${{ steps.secrets.outputs.SLACK_BOT_TOKEN }}
        with:
          channel-id: ${{ vars.NOTIF_CHANNEL_ID }}
          payload: |
            {
              "attachments": [
                {
                  "mrkdwn_in": ["text"],
                  "color": "${{ fromJSON('["#36a64f", "#A30002"]')[job.status != 'success'] }}",
                  "author_name": "${{github.actor}}",
                  "author_link": "https://github.com/${{github.actor}}",
                  "author_icon": "https://github.com/${{github.actor}}.png",
                  "title": "Airflow déployé",
                  "title_link": "https://github.com/${{github.repository}}/actions/runs/${{github.run_id}}",
                  "text": "Airflow a été déployé sur l'environnement ${{ inputs.AIRFLOW_NAME }}"
                }
              ],
              "unfurl_links": false,
              "unfurl_media": false
            }
