# Airflow Orchestration

Repository for orchestrating workflows using Airflow with Kubernetes deployment.

---

## ğŸ“ Project Structure

* **`airflow/`** â€“ Contains Airflow configuration files.
* **`dags/`** â€“ DAGs deployed to a GCS bucket, accessible by the Airflow instance. (See [DAG Directory Structure](#dags-directory-structure))
* **`k8s-airflow/`** â€“ Kubernetes configs to launch Airflow. (See [Related Notion Doc](https://www.notion.so/passcultureapp/AIRFLOW-Kubernetes-1a4ad4e0ff988184b503ec43c9dd2691))
* **`plugins/`** â€“ Custom Airflow plugins (e.g. dbt documentation).
* **`tests/`** â€“ Unit and integration tests.

---

## ğŸ“‚ DAGs Directory Structure

Inside the `dags/` directory:

* **`jobs/`** â€“ DAG definition files.
* **`dependencies/`** â€“ DAG dependency modules.
* **`common/`** â€“ Shared components like hooks, macros, operators.
* **`data_gcp_dbt/`** â€“ Files related to dbt integration with GCP.

---

## ğŸš€ DAG Deployment & Execution (Kubernetes)

### ğŸ”„ Automatic Deployment

When merging to `master` or `production`, DAGs are automatically deployed to Airflow using GitHub Actions.
See [CD Documentation](../README.md#cd) for more.

Deployment process:

* Updates modified files in the bucket `airflow-data-bucket-{ENV}`.
* Confirms that Airflow successfully loads the DAGs.

### ğŸ› ï¸ Trigger DAGs Manually

To manually access or trigger DAGs, use the following Airflow instances:

* **EHP**: [https://airflow-{env}.data.ehp.passculture.team](https://airflow-{env}.data.ehp.passculture.team)
* **Production**: [https://airflow.data.passculture.team](https://airflow.data.passculture.team)

---

## ğŸ§ª Local Airflow Setup (with Docker)

To run Airflow locally for development:

---

### ğŸ” Prerequisites: GCP Auth & Environment Variables

1. **If behind a Netskope proxy**:

   * Locate the **bundled/combined** certificate file for your machine.
     If unsure, check this [Notion page](https://www.notion.so/passcultureapp/Proxyfication-des-outils-du-pass-d1f0da09eafb4158904e9197bbe7c1d4?pvs=4#10cad4e0ff98805ba61efcea26075d65).
   * Place it in:

     ```
     /airflow/etc/nscacert_combined.pem
     ```

2. **Environment file setup**:

   * Copy `.env.template` to `orchestration/.env`
   * Update variable values accordingly.

#### ğŸ› ï¸ Environment Variable Tips

* `_AIRFLOW_WWW_USER_USERNAME` / `_AIRFLOW_WWW_USER_PASSWORD`: set arbitrarily
* `AIRFLOW__CORE__FERNET_KEY` / `AIRFLOW__WEBSERVER__SECRET_KEY`: arbitrary strings
* `NETWORK_MODE`:

  * `"proxy"` if using a proxy
  * `"default"` otherwise
* For environment:

  * **dev**:

    ```env
    ENV_SHORT_NAME=dev
    GOOGLE_CLOUD_PROJECT=passculture-data-ehp
    ```

  * **stg**:

    ```env
    ENV_SHORT_NAME=stg
    GOOGLE_CLOUD_PROJECT=passculture-data-ehp
    ```

---

## ğŸ—ï¸ Build & Run

### ğŸ”§ Build the Docker image

âš ï¸ This will ask you to authenticate with GCP with your gadmin account.

```sh
make build
```

### â–¶ï¸ Start Airflow

âš ï¸ This will ask you to authenticate with GCP with your gadmin account.

```sh
make start
```

Then access the Airflow UI at:

```sh
http://localhost:8080
```

---

## ğŸ›‘ Stop Services

```sh
make stop
```

---

## ğŸ” Update Environment Variables

After modifying `orchestration/.env`, rebuild with:

```sh
make build_with_cache
```

---

## ğŸ§¹ Troubleshooting

### View logs from containers

```sh
make show_airflow_logs
```

---

## ğŸ³ Dockerfile Tips

### Build with specific `NETWORK_MODE`

```sh
docker build -t <docker-image-name> <dockerfile-path> --build-arg NETWORK_MODE=<proxy|default>
```

### Build a specific target (multi-stage)

```sh
docker build --no-cache -f Dockerfile --target <target-name> .
```
