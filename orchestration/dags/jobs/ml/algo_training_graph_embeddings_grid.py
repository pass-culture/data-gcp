import json
from datetime import datetime, timedelta

from airflow import DAG
from airflow.models import Param
from airflow.operators.empty import EmptyOperator
from airflow.providers.google.cloud.operators.bigquery import (
    BigQueryInsertJobOperator,
)
from common import macros
from common.callback import on_failure_vm_callback
from common.config import (
    BIGQUERY_ML_GRAPH_RECOMMENDATION_DATASET,
    DAG_FOLDER,
    DAG_TAGS,
    ENV_SHORT_NAME,
    GCP_PROJECT_ID,
    INSTANCES_TYPES,
    ML_BUCKET_TEMP,
)
from common.operators.gce import (
    SSHGCEOperator,
)
from common.utils import get_airflow_schedule
from common.grid_search import GridDAG, ExecutionMode, ParameterSearchMode
from jobs.crons import SCHEDULE_DICT

# DAG metadata
DATE = "{{ ts_nodash }}"
DAG_NAME = "algo_training_graph_embeddings_grid"
DEFAULT_ARGS = {
    "start_date": datetime(2023, 5, 9),
    "on_failure_callback": on_failure_vm_callback,
    "retries": 0,
    "retry_delay": timedelta(minutes=2),
}

# GCE defaults
INSTANCE_NAME = f"grid-train-graph-embeddings-{ENV_SHORT_NAME}"
INSTANCE_TYPE = {
    "dev": "n1-standard-2",
    "stg": "n1-standard-16",
    "prod": "n1-standard-16",
}[ENV_SHORT_NAME]

# Paths
GCS_FOLDER_PATH = f"algo_training_{ENV_SHORT_NAME}/{DAG_NAME}_{DATE}"
STORAGE_BASE_PATH = f"gs://{ML_BUCKET_TEMP}/{GCS_FOLDER_PATH}"
BASE_DIR = "data-gcp/jobs/ml_jobs/graph_recommendation"
EMBEDDINGS_FILENAME = "embeddings.parquet"

# BigQuery tables
INPUT_TABLE_NAME = "item_with_metadata_to_embed"
EMBEDDING_TABLE_NAME = "graph_embedding"


# ML task chain
def ml_task_chain(params, instance_name, suffix):
    """
    Create Airflow tasks for a single parameter combination in a GridDAG.

    Args:
        params (dict): Parameter values for this combination; merged from the grid and
                        any common parameters by `build_grid`.
        instance_name (str): GCE instance where tasks run; assigned by `build_grid`.
        suffix (str): Unique identifier for task IDs and output files, generated by
                        `build_grid` to avoid collisions across combinations.

    Returns:
        List[BaseOperator]: Ordered tasks representing this combinationâ€™s workflow.
                            Internal structure (linear, parallel, or branched) is
                            preserved; `build_grid` handles connecting to VM lifecycle tasks.
    """
    config_json = json.dumps(params)

    train = SSHGCEOperator(
        task_id=f"train_{suffix}",
        instance_name=instance_name,
        base_dir=BASE_DIR,
        command=(
            "cli train-metapath2vec "
            "{{ params.experiment_name }} "
            f"{STORAGE_BASE_PATH}/raw_input/ "
            f"--output-embeddings {STORAGE_BASE_PATH}/{suffix}_{EMBEDDINGS_FILENAME} "
            "{% if params['train_only_on_10k_rows'] %} --nrows 10000 {% endif %} "
            f"--config '{config_json}'"
        ),
        deferrable=True,
        do_xcom_push=True,
    )

    evaluate = SSHGCEOperator(
        task_id=f"evaluate_{suffix}",
        instance_name=instance_name,
        base_dir=BASE_DIR,
        command=(
            "cli evaluate-metapath2vec "
            f"{{{{ ti.xcom_pull(task_ids='train_{suffix}') }}}} "
            f"{STORAGE_BASE_PATH}/raw_input/ "
            f"{STORAGE_BASE_PATH}/{suffix}_{EMBEDDINGS_FILENAME} "
            f"{STORAGE_BASE_PATH}/{suffix}_evaluation_metrics.csv "
            f"--output-scores-path {STORAGE_BASE_PATH}/{suffix}_evaluation_scores_details.parquet"
        ),
        deferrable=True,
    )

    return [train, evaluate]


# Grid parameters
metapaths = [
    (
        4
        * [
            ("book", "is_artist_id", "artist_id"),
            ("artist_id", "artist_id_of", "book"),
        ]
        + 4
        * [
            ("book", "is_gtl_label_level_4", "gtl_label_level_4"),
            ("gtl_label_level_4", "gtl_label_level_4_of", "book"),
        ]
        + 3
        * [
            ("book", "is_gtl_label_level_3", "gtl_label_level_3"),
            ("gtl_label_level_3", "gtl_label_level_3_of", "book"),
        ]
        + 2
        * [
            ("book", "is_gtl_label_level_2", "gtl_label_level_2"),
            ("gtl_label_level_2", "gtl_label_level_2_of", "book"),
        ]
        + [
            ("book", "is_gtl_label_level_1", "gtl_label_level_1"),
            ("gtl_label_level_1", "gtl_label_level_1_of", "book"),
        ]
    ),
    (
        3
        * [
            ("book", "is_gtl_label_level_4", "gtl_label_level_4"),
            ("gtl_label_level_4", "gtl_label_level_4_of", "book"),
        ]
        + 1
        * [
            ("book", "is_artist_id", "artist_id"),
            ("artist_id", "artist_id_of", "book"),
        ]
        + 3
        * [
            ("book", "is_gtl_label_level_3", "gtl_label_level_3"),
            ("gtl_label_level_3", "gtl_label_level_3_of", "book"),
        ]
        + 1
        * [
            ("book", "is_artist_id", "artist_id"),
            ("artist_id", "artist_id_of", "book"),
        ]
        + 3
        * [
            ("book", "is_gtl_label_level_2", "gtl_label_level_2"),
            ("gtl_label_level_2", "gtl_label_level_2_of", "book"),
        ]
        + 1
        * [
            ("book", "is_artist_id", "artist_id"),
            ("artist_id", "artist_id_of", "book"),
        ]
        + 1
        * [
            ("book", "is_gtl_label_level_1", "gtl_label_level_1"),
            ("gtl_label_level_1", "gtl_label_level_1_of", "book"),
        ]
        + 1
        * [
            ("book", "is_artist_id", "artist_id"),
            ("artist_id", "artist_id_of", "book"),
        ]
    ),
]

PARAM_GRID = {"embedding_dim": [32, 64, 128]}  # , "metapath": metapaths}

SHARED_PARAMS = {
    "base_dir": BASE_DIR,
}

with GridDAG(
    dag_id="algo_training_graph_embeddings_grid_search",
    description="Grid search training for graph embeddings",
    ml_task_fn=ml_task_chain,
    param_grid=PARAM_GRID,
    common_params=SHARED_PARAMS,
    search_mode=ParameterSearchMode.ORTHOGONAL,
    execution_mode=ExecutionMode.PARALLEL,
    n_vms=4,
    start_date=datetime(2023, 1, 1),
    schedule_interval=None,
    dagrun_timeout=timedelta(minutes=1200),
    user_defined_macros=macros.default,
    template_searchpath=DAG_FOLDER,
    render_template_as_native_obj=True,
    tags=[DAG_TAGS.DS.value, DAG_TAGS.VM.value],
    start_vm_kwargs={
        "preemptible": False,
        "instance_type": "{{ params.instance_type }}",
        "gpu_type": "{{ params.gpu_type }}",
        "gpu_count": "{{ params.gpu_count }}",
        "labels": {"job_type": "long_ml"},
    },
    install_deps_kwargs={
        "python_version": "3.12",
        "base_dir": BASE_DIR,
        "retries": 2,
    },
    params={
        "branch": Param(
            default="production" if ENV_SHORT_NAME == "prod" else "master",
            type="string",
        ),
        "instance_type": Param(
            default=INSTANCE_TYPE, enum=INSTANCES_TYPES["cpu"]["standard"]
        ),
        "instance_name": Param(default=INSTANCE_NAME, type="string"),
        "gpu_type": Param(
            default="nvidia-tesla-t4", enum=INSTANCES_TYPES["gpu"]["name"]
        ),
        "gpu_count": Param(default=1, enum=INSTANCES_TYPES["gpu"]["count"]),
        "experiment_name": Param(
            default="algo_training_graph_embeddings_v1", type="string"
        ),
        "train_only_on_10k_rows": Param(default=True, type="boolean"),
    },
) as dag:
    start = EmptyOperator(task_id="start")

    import_offer_as_parquet = BigQueryInsertJobOperator(
        project_id=GCP_PROJECT_ID,
        task_id="import_offer_as_parquet",
        configuration={
            "extract": {
                "sourceTable": {
                    "projectId": GCP_PROJECT_ID,
                    "datasetId": BIGQUERY_ML_GRAPH_RECOMMENDATION_DATASET,
                    "tableId": INPUT_TABLE_NAME,
                },
                "compression": None,
                "destinationUris": f"{STORAGE_BASE_PATH}/raw_input/data-*.parquet",
                "destinationFormat": "PARQUET",
            }
        },
    )

    start_grid, end_grid = dag.build_grid()

    start >> import_offer_as_parquet >> start_grid >> end_grid
